{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vihith123/RAG_Application/blob/main/NutriChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXTgUnkzPRW5"
      },
      "outputs": [],
      "source": [
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    #!pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XdHGXZqkbdh"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers sentence-transformers\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -U transformers sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjS6LA7gkbgQ"
      },
      "outputs": [],
      "source": [
        "# Download PDF file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document\n",
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "# Download PDF if it doesn't already exist\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "    # The URL of the PDF you want to download\n",
        "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "    # The local filename to save the downloaded file\n",
        "    filename = pdf_path\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Open a file in binary write mode and save the content to it\n",
        "        with open(filename, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"The file has been downloaded and saved as {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "    print(f\"File {pdf_path} exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih7xeggYkbjA"
      },
      "outputs": [],
      "source": [
        "import fitz # This is the PyMuPdf\n",
        "from tqdm.auto import tqdm # This Helps in faster iteration\n",
        "def text_formatter(text: str) ->str:\n",
        "  #Performs Minor operation on the text\n",
        "  cleaned_text = text.replace(\"\\n\",\" \").strip()\n",
        "  return cleaned_text\n",
        "\n",
        "# Now we will open the pdf\n",
        "def open_and_read_pdf(pdf_path: str) ->list:\n",
        "  doc = fitz.open(pdf_path) # opening a document\n",
        "  pages_texts = [] # this is where we store the text contained in the certain number of page\n",
        "  for page_number, page in tqdm(enumerate(doc)):\n",
        "    text = page.get_text()\n",
        "    text = text_formatter(text)\n",
        "    pages_texts.append({\"page_number\": page_number + 1,\n",
        "                        \"page_char_count\": len(text),\n",
        "                        \"page_word_count\": len(text.split(\" \")),\n",
        "                        \"page_sentence_count\": len(text.split(\". \")),\n",
        "                        \"Page_token_count\": len(text)/4,\n",
        "                        \"text\": text\n",
        "                        })\n",
        "  return pages_texts # Moved return statement outside the loop\n",
        "\n",
        "pages_texts = open_and_read_pdf(pdf_path = pdf_path)\n",
        "pages_texts[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgip4KE2kblp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pages_texts)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN-nk_V9kboZ"
      },
      "outputs": [],
      "source": [
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLcUrhoBkbtg"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "nlp.add_pipe(\"sentencizer\") # It breaks Sentences in a paragraph into Single Sentences\n",
        "doc = nlp(\"This is Vihith. I am a chess Player\")\n",
        "assert len(list(doc.sents))==2\n",
        "list(doc.sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jConPiBTkbvw"
      },
      "outputs": [],
      "source": [
        "for item in tqdm(pages_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "\n",
        "    # Make sure all sentences are strings\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    # Count the sentences\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDT8kyts6-j7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.sample(pages_texts,k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFxy7F20_mUC"
      },
      "outputs": [],
      "source": [
        "# Define split size to turn groups of sentences into chunks\n",
        "num_sentence_chunk_size = 10\n",
        "\n",
        "# Create a function that recursively splits a list into desired sizes\n",
        "def split_list(input_list: list,\n",
        "               slice_size: int) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
        "\n",
        "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
        "    \"\"\"\n",
        "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "# Loop through pages and texts and split sentences into chunks\n",
        "for item in tqdm(pages_texts):\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         slice_size=num_sentence_chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIV2xjoW6-mv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Split each chunk into its own item\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_texts):\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
        "        joined_sentence_chunk = \" \".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)  # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
        "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "        # Get stats about the chunk\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) // 4  # 1 token = ~4 characters\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "# How many chunks do we have?\n",
        "len(pages_and_chunks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roZlQJH96-qZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.sample(pages_and_chunks, k =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBw2Cgka6-tQ"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zouK6Rah6-v4"
      },
      "outputs": [],
      "source": [
        "min_chunk_char_count = 30\n",
        "pages_and_chunks_over_min_length = [\n",
        "    chunk for chunk in pages_and_chunks if chunk[\"chunk_char_count\"] >= min_chunk_char_count\n",
        "]\n",
        "\n",
        "print(f\"Number of chunks before filtering: {len(pages_and_chunks)}\")\n",
        "print(f\"Number of chunks after filtering (min char count >= {min_chunk_char_count}): {len(pages_and_chunks_over_min_length)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKui8WCXDpH2"
      },
      "outputs": [],
      "source": [
        "pages_and_chunks_over_min_length[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f34db6"
      },
      "source": [
        "### Set up your Google API Key\n",
        "\n",
        "To use the Gemini API, you'll need an API key. If you don't already have one, create a key in [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`. Then, the following code will retrieve and configure the API key for use with `google.generativeai`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d74415ee"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as genai\n",
        "# # from google.colab import userdata\n",
        "\n",
        "# # Retrieve API key from Colab secrets\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# # Configure the generative AI library\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# print(\"Google Generative AI API configured successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f12ae7"
      },
      "source": [
        "Now that the API is configured, you can use the `gemini-embedding-001` model to embed content. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "109de461"
      },
      "outputs": [],
      "source": [
        "# result = genai.embed_content(\n",
        "#     model=\"gemini-embedding-001\",\n",
        "#     content=\"What is the meaning of life?\"\n",
        "# )\n",
        "\n",
        "# print(\"Embedding for 'What is the meaning of life?':\")\n",
        "# print(result['embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHXPyKrCDpKN"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as genai\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Ensure your GOOGLE_API_KEY is set in Colab secrets\n",
        "# # This cell should be executed after the GOOGLE_API_KEY is properly configured\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# embedding_model_name = \"gemini-embedding-001\"\n",
        "# print(f\"Using embedding model: {embedding_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPOG9AGmDpM9"
      },
      "outputs": [],
      "source": [
        "# #import google.generativeai as genai\n",
        "# #from tqdm.auto import tqdm\n",
        "\n",
        "# # Ensure GOOGLE_API_KEY is configured (from the previous cell)\n",
        "# # gemini-embedding-001 is a cloud-based model, so no .to(\"cuda\") is needed\n",
        "\n",
        "# f#or item in tqdm(pages_and_chunks_over_min_length):\n",
        "#     # Embed content using the Gemini embedding model\n",
        "#   #  response = genai.embed_content(\n",
        "#        model=embedding_model_name,\n",
        "#    #     content=item[\"sentence_chunk\"]\n",
        "#     #)\n",
        "#     #item[\"embedding\"] = response['embedding']\n",
        "\n",
        "# #print(\"Embeddings generated successfully using Gemini embedding model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "df9148a6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the SentenceTransformer model locally\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)\n",
        "print(f\"SentenceTransformer model 'all-mpnet-base-v2' loaded successfully to {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f896f97f"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# The embedding_model (SentenceTransformer) is assumed to be loaded in a previous cell.\n",
        "\n",
        "for item in tqdm(pages_and_chunks_over_min_length):\n",
        "    # Embed content using the local SentenceTransformer model\n",
        "    # The model returns a tensor, convert to a list for easier storage in DataFrame later\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"], convert_to_tensor=False).tolist()\n",
        "\n",
        "print(\"Embeddings generated successfully using SentenceTransformer model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEzEW8NEDpPf"
      },
      "outputs": [],
      "source": [
        "text_chunk_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_length)\n",
        "embeddings_df_save_path = \"text_chunk_and_embeddings_df.csv\"\n",
        "\n",
        "text_chunk_and_embeddings_df.to_csv(embeddings_df_save_path, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NvJXtNpLX12"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLkLNm6wKuj3"
      },
      "outputs": [],
      "source": [
        "text_chunk_and_embeddings_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunk_and_embeddings_df_load.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VaaEDjfzKumv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Removed json import as it's no longer used for parsing this string format\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Import texts and embedding df\n",
        "text_chunks_and_embedding_df = pd.read_csv(\"text_chunk_and_embeddings_df.csv\")\n",
        "\n",
        "# Function to parse the string representation of a numpy array back into a numpy array\n",
        "def parse_embedding_string_to_array(embedding_str):\n",
        "    # Remove leading/trailing brackets, any ellipsis, and split by space.\n",
        "    # Example string in CSV: \"[ 0.0674242675  0.0902281404 -0.00509548886 ...]\"\n",
        "    # First, handle potential truncation by '...' if it's present, otherwise it might cause ValueError.\n",
        "    if '...' in embedding_str:\n",
        "        # Attempt to remove '...' and ensure proper closing bracket, if it was truncated.\n",
        "        # This is a heuristic and assumes '...' is at the end of meaningful numbers.\n",
        "        embedding_str = embedding_str.split('...')[0].strip() + ']'\n",
        "\n",
        "    # Remove outer brackets and any extra spaces before splitting\n",
        "    cleaned_str = embedding_str.strip('[] ')\n",
        "\n",
        "    # Split by space and convert each numeric part to float. Filter out any empty strings.\n",
        "    try:\n",
        "        float_list = [float(num_str) for num_str in cleaned_str.split() if num_str]\n",
        "        return np.array(float_list, dtype=np.float32)\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Could not fully parse embedding string: '{embedding_str}'. Returning empty array.\")\n",
        "        return np.array([], dtype=np.float32)\n",
        "\n",
        "# Apply the custom parsing function to the 'embedding' column\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(parse_embedding_string_to_array)\n",
        "\n",
        "# Filter out any rows where parsing might have failed or resulted in an incorrect embedding dimension\n",
        "# The embedding model 'all-mpnet-base-v2' produces 768-dimensional embeddings.\n",
        "expected_embedding_dim = 768\n",
        "text_chunks_and_embedding_df = text_chunks_and_embedding_df[\n",
        "    text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: len(x) == expected_embedding_dim)\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Convert embeddings to torch tensor and send to device (NumPy arrays are float64, torch tensors are float32 by default)\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d1l6ztMWKupQ"
      },
      "outputs": [],
      "source": [
        "text_chunks_and_embedding_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D223jNDBKurv"
      },
      "outputs": [],
      "source": [
        "embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKe1SezNKuum"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKyZ5u18DpR8"
      },
      "outputs": [],
      "source": [
        "query = \"macronutrients functions\"\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor = True)\n",
        "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
        "top_results_of_dot_scores = torch.topk(dot_scores, k=5)\n",
        "top_results_of_dot_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNgG2U8JVScn"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_wrapped(text,wrap_length=80):\n",
        "  wrapped_text = textwrap.wrap(text, wrap_length)\n",
        "  print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TNRwdW-VSfN"
      },
      "outputs": [],
      "source": [
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Results:\")\n",
        "# Loop through zipped together scores and indiciâ€‹es from torch.topk\n",
        "for score, idx in zip(top_results_of_dot_scores[0], top_results_of_dot_scores[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    # Print the page number too so we can reference the textbook further (and check the results)\n",
        "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awrShXpJVSh1"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)\n",
        "\n",
        "def retrieve_element(query: str,\n",
        "                     embeddings: torch.tensor,\n",
        "                     model: SentenceTransformer=embedding_model,\n",
        "                     n_of_resources_to_return: int = 5,\n",
        "                     ):\n",
        "  query_embedding = embedding_model.encode(query, convert_to_tensor = True)\n",
        "  dot_scores = util.dot_score(query_embedding,embeddings)[0] # Corrected from dot_scores to dot_score\n",
        "  scores, indices = torch.topk(input=dot_scores, k = n_of_resources_to_return)\n",
        "  return scores, indices\n",
        "\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
        "                                 n_resources_to_return: int=5):\n",
        "\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
        "\n",
        "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
        "    \"\"\"\n",
        "\n",
        "    scores, indices = retrieve_element(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_of_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    # Loop through zipped together scores and indices\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        # Print relevant sentence chunk (since the scores are in descending order,\n",
        "        # the most relevant chunk will be first)\n",
        "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
        "        # Print the page number too so we can reference the textbook further and check the results\n",
        "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28496561"
      },
      "outputs": [],
      "source": [
        "query = \"symptoms of pellagra\"\n",
        "scores, indices = retrieve_element(query=query,\n",
        "                                   embeddings=embeddings,\n",
        "                                   n_of_resources_to_return=5)\n",
        "scores, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70431b3c"
      },
      "outputs": [],
      "source": [
        "query = \"symptoms of pellagra\"\n",
        "print_top_results_and_scores(query=query,\n",
        "                                   embeddings=embeddings,\n",
        "                                   pages_and_chunks=pages_and_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL6ISUO0KN1U"
      },
      "outputs": [],
      "source": [
        "# Import the notebook login utility\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Log in using the token stored in Colab secrets\n",
        "# If HF_TOKEN is in Colab secrets and enabled for the notebook, this will use it automatically.\n",
        "# Otherwise, it will prompt you to paste your token.\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rOt23Fsai4N"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BT0E6gPVSm4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "# 1. Creating Quantization for config Model\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype = torch.float16)\n",
        "attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "# Define use_quantization_config (assuming it should be True given the setup)\n",
        "use_quantization_config = True\n",
        "\n",
        "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
        "model_id = \"google/gemma-2b-it\" # Uncommented and assigned a value\n",
        "print(f\"[INFO] Using model_id: {model_id}\")\n",
        "\n",
        "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "\n",
        "# 4. Instantiate the model\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_id,\n",
        "    torch_dtype=torch.float16,  # datatype to use, we want float16\n",
        "    quantization_config=quantization_config if use_quantization_config else None,\n",
        "    low_cpu_mem_usage=False,  # use full memory\n",
        "    attn_implementation=attn_implementation  # which attention version to use\n",
        ")\n",
        "\n",
        "if not use_quantization_config:  # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
        "    llm_model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSy45s8VVSpO"
      },
      "outputs": [],
      "source": [
        "llm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZtXBV90fKCr"
      },
      "outputs": [],
      "source": [
        "def get_model_param(model: torch.nn.Module):\n",
        "  return sum([param.numel() for param in model.parameters()])\n",
        "\n",
        "get_model_param(llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "545cH4NUfKFa"
      },
      "outputs": [],
      "source": [
        "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
        "print(f\"Input text:\\n{input_text}\")\n",
        "\n",
        "# Create prompt template for instruction-tuned model\n",
        "dialogue_template = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": input_text}\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                        tokenize=False,  # Keep as raw text (not tokenized)\n",
        "                                        add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7wUGsGYfKIE"
      },
      "outputs": [],
      "source": [
        "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
        "\n",
        "# Generate outputs passed on the tokenized input\n",
        "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "outputs = llm_model.generate(**input_ids,\n",
        "                             max_new_tokens=256)  # define the maximum number of new tokens to create\n",
        "\n",
        "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxC2vl6KfKKq"
      },
      "outputs": [],
      "source": [
        "# Decode the output tokens to text\n",
        "outputs_decoded = tokenizer.decode(outputs[0])\n",
        "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SbeOggIfKNb"
      },
      "outputs": [],
      "source": [
        "# Nutrition-style questions generated with GPT4\n",
        "gpt4_questions = [\n",
        "\"What are the macronutrients, and what roles do they play in the human body?\",\n",
        "\"How do vitamins and minerals differ in their roles and importance for health?\",\n",
        "\"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
        "\"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
        "\"Explain the concept of energy balance and its importance in weight management.\"\n",
        "]\n",
        "\n",
        "# Manually created question list\n",
        "manual_questions = [\n",
        "\"How often should infants be breastfed?\",\n",
        "\"What are symptoms of pellagra?\",\n",
        "\"How does saliva help with digestion?\",\n",
        "\"What is the RDI for protein per day?\",\n",
        "\"water soluble vitamins\"\n",
        "]\n",
        "\n",
        "query_list = gpt4_questions + manual_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPqeNx4-hI1e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "query = random.choice(query_list)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get just the scores and indices of top related results\n",
        "scores, indices = retrieve_element(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "\n",
        "scores, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAHs8nPuhI4O"
      },
      "outputs": [],
      "source": [
        "def prompt_formatter(query: str,\n",
        "                     context_items: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Augments query with text-based context from context_items.\n",
        "    \"\"\"\n",
        "    # Join context items into one dotted paragraph\n",
        "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "    # Create a base prompt with examples to help the model\n",
        "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
        "    # We could also write this in a text file and import it in if we wanted.\n",
        "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "Don't return the thinking, only return the answer.\n",
        "Make sure your answers are as explanatory as possible.\n",
        "Use the following examples as reference for the ideal answer style.\n",
        "\\nExample 1:\n",
        "Query: What are the fat-soluble vitamins?\n",
        "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fat in the diet and can be stored in the body's fatty tissue.\n",
        "\\nExample 2:\n",
        "Query: What are the causes of type 2 diabetes?\n",
        "Answer: Type 2 diabetes is often associated with overeating, particularly the overconsumption of calories leading to obesity. Factors like lack of physical activity and genetic predisposition also contribute.\n",
        "\\nExample 3:\n",
        "Query: What is the importance of hydration for physical performance?\n",
        "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and enabling muscle contractions.\n",
        "\\nNow use the following context items to answer the user query:\n",
        "{context}. The context contains the answer. Look carefully and extract relevant information.\n",
        "\\nRelevant passages: <extract relevant passages from the context here>\n",
        "User query: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "    # Update base prompt with context items and query\n",
        "    base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    # Create prompt template for instruction-tuned model\n",
        "    dialogue_template = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": base_prompt}\n",
        "   ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                       tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ2hj_9dhI63"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "query = random.choice(query_list)\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get relevant resources\n",
        "scores, indices = retrieve_element(query=query,\n",
        "                                             embeddings=embeddings)\n",
        "\n",
        "# Create a list of context items\n",
        "context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "# Format prompt with context items\n",
        "prompt = prompt_formatter(query=query,\n",
        "                         context_items=context_items)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zccpmBehI9F"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate an output of tokens\n",
        "outputs = llm_model.generate(**input_ids,\n",
        "                            temperature=0.7, # lower temperature = less random\n",
        "                            do_sample=True, # whether or not to use sampling\n",
        "                            max_new_tokens=256) # how many new tokens to generate\n",
        "\n",
        "# Turn the output tokens into text\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZNdWAgShJDN"
      },
      "outputs": [],
      "source": [
        "def ask(query,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=512,\n",
        "        format_answer_text=True,\n",
        "        return_answer_only=True):\n",
        "    \"\"\"\n",
        "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get just the scores and indices of top related results\n",
        "    scores, indices = retrieve_element(query=query,\n",
        "                                                 embeddings=embeddings)\n",
        "\n",
        "    # Create a list of context items\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    # Add score to context item\n",
        "    for i, item in enumerate(context_items):\n",
        "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
        "\n",
        "    # Format the prompt with context items\n",
        "    prompt = prompt_formatter(query=query,\n",
        "                            context_items=context_items)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "   # Turn the output tokens into text\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    if format_answer_text:\n",
        "        # Replace special tokens and unnecessary help message\n",
        "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
        "\n",
        "    # Only return the answer without the context items\n",
        "    if return_answer_only:\n",
        "        return output_text\n",
        "\n",
        "    return output_text, context_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeXqzjG5PxBt"
      },
      "outputs": [],
      "source": [
        "query = random.choice(query_list)\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Answer query with context and return context\n",
        "answer, context_items = ask(query=query,\n",
        "                            temperature=0.7,\n",
        "                            max_new_tokens=512,\n",
        "                            return_answer_only=False)\n",
        "\n",
        "print(f\"\\nAnswer:\\n\")\n",
        "print_wrapped(answer)\n",
        "print(f\"Context items:\")\n",
        "context_items\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EswRJqfxPxFF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBg8ISNBPxK4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT5H6lfpPxQB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMuRjnaAmrdiqvCid85WDpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}